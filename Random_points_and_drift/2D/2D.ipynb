{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map:\n",
    "    def __init__(self, blocks_num, random_seed) -> None:\n",
    "        self.size = np.array([100, 100])\n",
    "        self.dsize = 1 / self.size\n",
    "        self.dagsize = 1 / np.linalg.norm(self.size)\n",
    "        self.random_seed = random_seed\n",
    "        self.blocks_num = blocks_num\n",
    "        self.point_size = 2\n",
    "        self.init_size = self.point_size * 5\n",
    "        self._random_map(random_seed)\n",
    "        return\n",
    "\n",
    "    def _random_map(self, random_seed) -> None:\n",
    "        np.random.seed(random_seed)\n",
    "        self.drift = np.random.rand(2)*2-1\n",
    "        self.drift = self.drift/np.linalg.norm(self.drift) *0.1\n",
    "\n",
    "        self.blocks = np.zeros((self.blocks_num, 2))\n",
    "        self.BEGIN = np.random.rand(2) * self.size\n",
    "        self.END = np.random.rand(2) * self.size\n",
    "\n",
    "        i = 0\n",
    "        while i < self.blocks_num:\n",
    "            point = np.random.rand(2) * self.size\n",
    "            fitted = True\n",
    "            d1 = np.linalg.norm(point-self.BEGIN)\n",
    "            d2 = np.linalg.norm(point-self.END)\n",
    "            if d1 <= self.init_size or d2 <= self.init_size:\n",
    "                fitted = False\n",
    "\n",
    "            for j in range(i):\n",
    "                d = np.linalg.norm(point-self.blocks[j])\n",
    "                if d <= self.init_size:\n",
    "                    fitted = False\n",
    "\n",
    "            if fitted:\n",
    "                self.blocks[i] = point\n",
    "                i += 1\n",
    "\n",
    "        self.vd_blocks = self.blocks.flatten()\n",
    "        return\n",
    "\n",
    "    def reRandom(self):\n",
    "        fitted = False\n",
    "        while not fitted:\n",
    "            fitted = True\n",
    "            self.BEGIN = np.random.rand(2) * self.size\n",
    "            if np.min(np.linalg.norm(self.blocks - self.BEGIN, axis=1)) <= self.init_size:\n",
    "                fitted = False\n",
    "        \n",
    "        fitted = False\n",
    "        while not fitted:\n",
    "            fitted = True\n",
    "            self.END = np.random.rand(2) * self.size\n",
    "            if np.linalg.norm(self.END - self.BEGIN) <= self.init_size:\n",
    "                fitted = False\n",
    "            if np.min(np.linalg.norm(self.blocks - self.END, axis=1)) <= self.init_size:\n",
    "                fitted = False\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    pos = np.zeros(2)\n",
    "    vel = np.zeros(2)\n",
    "\n",
    "    def __init__(self, Map) -> None:\n",
    "        self.map = Map\n",
    "        self.reset()\n",
    "        self.max_step = 500\n",
    "        return\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.pos = self.map.BEGIN.copy()\n",
    "        self.vel = np.zeros(2)\n",
    "        self.steps = 0\n",
    "        self.old_pos = self.pos.copy()\n",
    "        self.old_vel = np.zeros(2)\n",
    "        return\n",
    "\n",
    "    def step(self, act):\n",
    "        # 保存旧状态\n",
    "        self.old_vel = self.vel.copy()\n",
    "        self.old_pos = self.pos.copy()\n",
    "\n",
    "        # 方向和力度\n",
    "        dir = np.array([np.cos(act[1]*np.pi), np.sin(act[1]*np.pi)])\n",
    "        f = act[0] + 1\n",
    "        self.vel = dir * f *0.2 + self.vel *0.8\n",
    "        self.pos += self.vel + (np.random.rand(2) *2-1)*0.01 + self.map.drift\n",
    "\n",
    "        target = np.linalg.norm(self.map.END - self.pos)\n",
    "        R1 =  -target *self.map.dagsize         # usually in[-0.8,0]\n",
    "\n",
    "        R2 = f *-0.001                         \n",
    "\n",
    "        R3 = 0                                  # 0 or 10\n",
    "        arrive = False\n",
    "        if target <= self.map.point_size:\n",
    "            R3 = 10\n",
    "            arrive = True\n",
    "\n",
    "        min_dis = np.min(np.linalg.norm(self.map.blocks - self.pos, axis=1))\n",
    "        R4 = min_dis *self.map.dagsize *0.002       \n",
    "        if min_dis <= self.map.point_size:\n",
    "            R4 -= 10                            # -10 or 0\n",
    "            \n",
    "        if (self.pos < 0).any() or (self.pos > self.map.size).any():\n",
    "            R4 -= 0.1                           # -0.1 or 0\n",
    "            self.pos = self.old_pos.copy()\n",
    "            self.vel = self.old_vel.copy()\n",
    "\n",
    "        return R1+R2+R3+R4, arrive, min_dis\n",
    "\n",
    "    def get_state(self):\n",
    "        tmp = np.multiply(self.map.blocks, self.map.dsize)\n",
    "        np.random.shuffle(tmp)\n",
    "        return np.concatenate((\n",
    "            tmp.flatten(),\n",
    "            self.map.drift,\n",
    "            self.pos *self.map.dsize,\n",
    "            self.vel *self.map.dsize,\n",
    "            self.old_pos *self.map.dsize,\n",
    "            self.old_vel *self.map.dsize,\n",
    "            self.map.END *self.map.dsize\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Net(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Actor_Net, self).__init__()\n",
    "        self.liner1 = nn.Linear(dim, 512)\n",
    "        self.liner2 = nn.Linear(512, 512)\n",
    "        self.liner3 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.liner1(x))\n",
    "        x = F.relu(self.liner2(x))\n",
    "        return torch.tanh(self.liner3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Net(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Critic_Net, self).__init__()\n",
    "        self.actliner = nn.Linear(2, 256)\n",
    "        self.xliner = nn.Linear(dim, 256)\n",
    "        self.liner = nn.Linear(512, 512)\n",
    "        self.liner2 = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        act = F.relu(self.actliner(act))\n",
    "        x = F.relu(self.xliner(x))\n",
    "        z = torch.cat((x, act), dim = 1)\n",
    "        z = F.relu(self.liner(z))\n",
    "        return self.liner2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_num = 20\n",
    "\n",
    "AMap = Map(blocks_num, 42)\n",
    "agent = Agent(AMap)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dim = agent.get_state().shape[0]\n",
    "learning_rate = 2e-4\n",
    "wd = 1e-4\n",
    "\n",
    "actor_net = Actor_Net(dim).to(device)\n",
    "T_actor_net = Actor_Net(dim).to(device)\n",
    "T_actor_net.load_state_dict(actor_net.state_dict())\n",
    "actor_net_optim = torch.optim.Adam(actor_net.parameters(), lr = learning_rate, weight_decay = wd)\n",
    "\n",
    "critic_net_1 = Critic_Net(dim).to(device)\n",
    "T_critic_net_1 = Critic_Net(dim).to(device)\n",
    "T_critic_net_1.load_state_dict(critic_net_1.state_dict())\n",
    "critic_net_1_optim = torch.optim.Adam(critic_net_1.parameters(), lr = learning_rate, weight_decay = wd)\n",
    "\n",
    "critic_net_2 = Critic_Net(dim).to(device)\n",
    "T_critic_net_2 = Critic_Net(dim).to(device)\n",
    "T_critic_net_2.load_state_dict(critic_net_2.state_dict())\n",
    "critic_net_2_optim = torch.optim.Adam(critic_net_2.parameters(), lr = learning_rate, weight_decay = wd)\n",
    "\n",
    "\n",
    "exp_pool = ReplayBuffer(dim, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# fig = go.Figure(data=[go.Scatter3d(x=AMap.blocks[:, 0],\n",
    "#                 y=AMap.blocks[:, 1], z=AMap.blocks[:, 2], mode='markers')])\n",
    "# fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "#                   title='3D Scatter Plot')\n",
    "# fig.show()\n",
    "print(AMap.BEGIN, AMap.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in trange(100):\n",
    "    agent.reset()\n",
    "    for __ in range(100):\n",
    "        state1 = agent.get_state()\n",
    "        act = np.random.rand(2)*2-1\n",
    "        reward, arrive, _ = agent.step(act)\n",
    "        state2 = agent.get_state()\n",
    "        exp_pool.add(state1, act, state2, reward, arrive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.996\n",
    "\n",
    "\n",
    "\n",
    "def soft_update(target, source, t):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_((1 - t) * target_param.data + t * source_param.data)\n",
    "\n",
    "def train(update_actor = True):\n",
    "    #exp = exp_pool.Exp_list[0]\n",
    "    state1, act, state2, reward, _ = exp_pool.sample(618)\n",
    "\n",
    "    pred_act = T_actor_net(state2)*0.8 + torch.randn(2).clamp(-1, 1).to(device) *0.2\n",
    "\n",
    "    pred_q1 = T_critic_net_1(state2, pred_act)\n",
    "    pred_q2 = T_critic_net_2(state2, pred_act)\n",
    "\n",
    "    td_target = (reward + gamma * torch.min(pred_q1, pred_q2)).detach()\n",
    "\n",
    "    td_error_1 = F.mse_loss(critic_net_1(state1, act), td_target)\n",
    "    td_error_2 = F.mse_loss(critic_net_2(state1, act), td_target)\n",
    "\n",
    "    critic_net_1_optim.zero_grad()\n",
    "    td_error_1.backward()\n",
    "    #torch.nn.utils.clip_grad_value_(critic_net_1.parameters(), clip_value=1)\n",
    "    critic_net_1_optim.step()\n",
    "\n",
    "    critic_net_2_optim.zero_grad()\n",
    "    td_error_2.backward()\n",
    "    #torch.nn.utils.clip_grad_value_(critic_net_2.parameters(), clip_value=1)\n",
    "    critic_net_2_optim.step()\n",
    "\n",
    "    if update_actor:\n",
    "        q = -critic_net_1(state1, actor_net(state1)).mean()\n",
    "\n",
    "        actor_net_optim.zero_grad()\n",
    "        q.backward()\n",
    "        #torch.nn.utils.clip_grad_value_(actor_net.parameters(), clip_value=1)\n",
    "        actor_net_optim.step()\n",
    "\n",
    "        soft_update(T_actor_net, actor_net, 0.005)\n",
    "        soft_update(T_critic_net_1, critic_net_1, 0.005)\n",
    "        soft_update(T_critic_net_2, critic_net_2, 0.005)\n",
    "        \n",
    "        return td_error_1, td_error_2, q\n",
    "\n",
    "    return td_error_1, td_error_2\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "maps = [Map(blocks_num, seed) for seed in [11, 22, 33, 44, 55, 66, 77, 88, 99]]\n",
    "agents = [Agent(m) for m in maps]\n",
    "\n",
    "# 统计量\n",
    "rewards = []\n",
    "floss = []\n",
    "first = []\n",
    "min_d = []\n",
    "min_d_average = []\n",
    "vel_average = []\n",
    "\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(episode, agent):\n",
    "    agent.reset()\n",
    "\n",
    "    # 需统计量记录\n",
    "    episode_reward = 0\n",
    "    fsum = 0\n",
    "    first_arrive = agent.max_step\n",
    "    close_block = 1e8\n",
    "    close = np.zeros(agent.max_step)\n",
    "    vel_con = np.zeros(agent.max_step)\n",
    "    # 位置\n",
    "    positions = [agent.pos.copy()]\n",
    "\n",
    "    for step in trange(agent.max_step):\n",
    "        state1 = agent.get_state()\n",
    "\n",
    "        act = actor_net(torch.FloatTensor(state1).to(device))\n",
    "        act = act.cpu().detach().numpy()\n",
    "        if np.random.random() < 0.2:\n",
    "            act = act * 0.5 + (np.random.rand(2)*2-1) * 0.5\n",
    "        reward, arrive, min_dis = agent.step(act)\n",
    "        state2 = agent.get_state()\n",
    "        exp_pool.add(state1, act, state2, reward, arrive)\n",
    "\n",
    "        # 需统计量记录\n",
    "        episode_reward += reward\n",
    "        fsum += act[0]+1\n",
    "        if arrive: first_arrive = min(first_arrive, step)\n",
    "        close_block = min(close_block, min_dis)\n",
    "        close[step] = min_dis\n",
    "        vel_con[step] = np.linalg.norm(agent.vel)\n",
    "        # 存储位置\n",
    "        positions.append(agent.pos.copy())  \n",
    "\n",
    "    positions = np.array(positions)\n",
    "\n",
    "    # 记录\n",
    "    rewards.append(episode_reward)\n",
    "    floss.append(fsum)\n",
    "    first.append(first_arrive)\n",
    "    min_d.append(close_block)\n",
    "    min_d_average.append(np.average(close))\n",
    "    vel_average.append(np.average(vel_con))\n",
    "    \n",
    "    print('Test ----- episode = {:4d} Reward = {:5.3f}'.format(episode, episode_reward))\n",
    "\n",
    "    if episode % 2000 == 0:\n",
    "        # 绘制智能体路径\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(positions[:, 0], positions[:, 1], label='Agent Path')\n",
    "        ax.scatter(agent.map.BEGIN[0], agent.map.BEGIN[1], color='b', label='Begin', s=20)\n",
    "        ax.scatter(agent.map.END[0], agent.map.END[1], color='r', label='Goal', s=20)\n",
    "        ax.scatter(agent.map.blocks[:, 0], agent.map.blocks[:, 1], color='k', label='Obstacle', s=10)\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.legend()\n",
    "        plt.title('Test Episode {:4d}, Reward {:.3f}'.format(episode, episode_reward))\n",
    "        # plt.show()\n",
    "\n",
    "        plt.savefig(f'./img/pic-{episode}.png')\n",
    "        torch.save(actor_net, f'./model/actor_net{episode}.pt')\n",
    "        torch.save(critic_net_1, f'./model/critic_net{episode}.pt')\n",
    "        np.save('rewards', rewards)\n",
    "        np.save('floss', floss)\n",
    "        np.save('first', first)\n",
    "        np.save('min_d', min_d)\n",
    "        np.save('min_d_avg', min_d_average)\n",
    "        np.save('vel_avg', vel_average)\n",
    "\n",
    "\n",
    "for i in range(160001):\n",
    "    for _ in range(5):\n",
    "        train(False)\n",
    "    train(True)\n",
    "    if (i % 20 == 0):\n",
    "        count += 1\n",
    "        test(i, agents[count % 9])\n",
    "        agents[count % 9].map.reRandom()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
